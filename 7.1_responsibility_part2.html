<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>7.1 AI pursuing its own goals in conflict with human goals or values - Responsibility</title>
    <link href="https://fonts.googleapis.com/css2?family=Figtree:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Figtree', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background-color: #ffffff;
            color: #000000;
            line-height: 1.3;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;        }

        h1 {
            text-align: center;
            margin-bottom: 8px;
            color: #000000;
            font-weight: 600;
            font-size: 18px;
        }

        .legend {
            text-align: center;
            font-size: 12px;
            color: #888888;
            font-style: italic;
            margin-bottom: 12px;
            padding: 8px;
            background-color: #f9f9f9;
            border-radius: 5px;
            border: 1px solid #e0e0e0;
        }

        .selection-title {


            text-align: center;


            font-size: 14px;


            font-weight: 600;


            color: #666666;


            margin-bottom: 10px;


        }



        .nav-pills {
            display: flex;
            flex-wrap: wrap;
            gap: 4px;
            margin-bottom: 15px;
            justify-content: center;
        }

        .nav-pill {
            background: #f8f9fa;
            border: 1px solid #e0e0e0;
            border-radius: 25px;
            padding: 12px 20px;
            cursor: pointer;
            font-family: 'Figtree', sans-serif;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            color: #000000;
        }

        .nav-pill:hover {
            background: #e9ecef;
            border-color: #000000;
        }

        .nav-pill.active {
            background: #000000;
            color: white;
            border-color: #000000;
        }

        .actor-section {
            display: none;
        }

        .actor-section.active {
            display: block;
        }

        .content-grid {
            display: flex;
            width: 100%;
            gap: 4px;
        }

        .content-column {
            background: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;        }

        .criteria-header {
            font-size: 12px;
            font-weight: 600;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid;
        }

        .criteria-header.higher {
            color: #FF0000;
            border-bottom-color: #FF0000;
        }

        .criteria-header.lower {
            color: #2E5C8A;
            border-bottom-color: #2E5C8A;
        }

        .summary-section {
            margin-bottom: 20px;
        }

        .summary-text {
            margin-bottom: 15px;
            font-weight: 500;
            color: #000000;
            font-size: 15px;
        }

        .quote-details {
            margin-top: 15px;
        }

        .quote-toggle {
            cursor: pointer;
            color: #000000;
            font-weight: 500;
            font-size: 16px;
            background-color: #ffff00;
            padding: 10px 15px;
            border-radius: 4px;
            display: inline-block;
        }

        .quote-toggle:hover {
            color: #333333;
        }

        .quote-list {
            margin-top: 15px;
            padding-left: 20px;
        }

        .quote-list li {
            margin-bottom: 12px;
            font-size: 16px;
            padding: 10px 15px;
            line-height: 1.3;
            color: #000000;
        }

        @media (max-width: 768px) {
            .content-grid {
                gap: 4px;
            }

            .selection-title {


                text-align: center;


                font-size: 14px;


                font-weight: 600;


                color: #666666;


                margin-bottom: 10px;


            }



            .nav-pills {
                justify-content: flex-start;
            }

            .nav-pill {
                font-size: 16px;
                padding: 4px 8px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>7.1 AI pursuing its own goals in conflict with human goals or values - Responsibility</h1>


        <div class="selection-title">Select an actor:</div>
                <div class="nav-pills">
            <button class="nav-pill active" data-target="AIDeveloperSpecializedAI">
                AI Developer (Specialized AI)
            </button>
            <button class="nav-pill" data-target="AIInfrastructureProvider">
                AI Infrastructure Provider
            </button>
            <button class="nav-pill" data-target="AffectedStakeholder">
                Affected Stakeholder
            </button>
        </div>

                <div class="content-sections">
<div class="actor-section active" id="AIDeveloperSpecializedAI">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Only one expert commented: "AI Deployers are responsible for the actions an AI takes, including putting safeguards around all model interactions. Alignment is their obligation, and in deploying a solution it should certainly be their capability. Model developers have the causal influence to create human-aligned models, but research shows that prompting can 'misalign' all state-of-the-art models. Capability for guardrails lies with deployers, not in model development."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Deployers are responsible for the actions an AI takes, including putting safeguards around all model interactions.  Alignment is their obligation, and in deploying a solution it should certainly be their capability.  Model developers have the causal influence to create human-aligned models, but research shows that prompting can 'misalign' all state-of-the-art models.  Capability for guardrails lies with deployers, not in model development."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Narrow systems like those used to predict equipment maintenance do not have a clear path to misalignmentâ€”such narrow systems simply do not ingest enough data or know enough about the world to have misaligned goals and act on them. Specialized AI always has clear narrow goals so they are not responsible for this risk.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"I don't see a clear path to misaligned narrow systems like those used to predict equipment maintenance.  Such narrow systems simply do not ingest enough data/know enough about the world to have misaligned goals and act on them... Therefore, I think they are less responsible for risk 7.1"</li>                    <li>"Specialized AI always have clear narrow goals so they are not responsible for this risk."</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
<div class="actor-section" id="AIInfrastructureProvider">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Infrastructure providers, if they take misalignment seriously, are some of the only people who can implement hardware design choices that would allow some governance actors to be effective (e.g., verifiable inference). Some argue AI infrastructure providers are responsible for important tasks like misalignment monitoring.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"Infrastructure providers, if they take misalignment seriously, are some of the only people who can implement hardware design choices that would allow some governance actors to be effective (e.g. verifiable inference)."</li>                    <li>"I remain of the view that AI infrastructure providers bear as least as much responsibility as specialised AI developers for something as fundamental as misalignment monitoring."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
<div class="actor-section" id="AffectedStakeholder">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AIDeveloperSpecializedAI">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Only one expert commented: "AI Deployers are responsible for the actions an AI takes, including putting safeguards around all model interactions. Alignment is their obligation, and in deploying a solution it should certainly be their capability. Model developers have the causal influence to create human-aligned models, but research shows that prompting can 'misalign' all state-of-the-art models. Capability for guardrails lies with deployers, not in model development."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Deployers are responsible for the actions an AI takes, including putting safeguards around all model interactions.  Alignment is their obligation, and in deploying a solution it should certainly be their capability.  Model developers have the causal influence to create human-aligned models, but research shows that prompting can 'misalign' all state-of-the-art models.  Capability for guardrails lies with deployers, not in model development."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Narrow systems like those used to predict equipment maintenance do not have a clear path to misalignmentâ€”such narrow systems simply do not ingest enough data or know enough about the world to have misaligned goals and act on them. Specialized AI always has clear narrow goals so they are not responsible for this risk.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"I don't see a clear path to misaligned narrow systems like those used to predict equipment maintenance.  Such narrow systems simply do not ingest enough data/know enough about the world to have misaligned goals and act on them... Therefore, I think they are less responsible for risk 7.1"</li>                    <li>"Specialized AI always have clear narrow goals so they are not responsible for this risk."</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AIDeployer">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Only one expert commented: "AI Deployers are responsible for the actions an AI takes, including putting safeguards around all model interactions. Alignment is their obligation, and in deploying a solution it should certainly be their capability. Model developers have the causal influence to create human-aligned models, but research shows that prompting can 'misalign' all state-of-the-art models. Capability for guardrails lies with deployers, not in model development."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Deployers are responsible for the actions an AI takes, including putting safeguards around all model interactions.  Alignment is their obligation, and in deploying a solution it should certainly be their capability.  Model developers have the causal influence to create human-aligned models, but research shows that prompting can 'misalign' all state-of-the-art models.  Capability for guardrails lies with deployers, not in model development."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AIInfrastructureProvider">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Infrastructure providers, if they take misalignment seriously, are some of the only people who can implement hardware design choices that would allow some governance actors to be effective (e.g., verifiable inference). Some argue AI infrastructure providers are responsible for important tasks like misalignment monitoring.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"Infrastructure providers, if they take misalignment seriously, are some of the only people who can implement hardware design choices that would allow some governance actors to be effective (e.g. verifiable inference)."</li>                    <li>"I remain of the view that AI infrastructure providers bear as least as much responsibility as specialised AI developers for something as fundamental as misalignment monitoring."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AIUser">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AffectedStakeholder">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AIGovernanceActor">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> AI governance actors are the only actors who can efficiently defuse race dynamics (particularly internationally) and provide strong safety requirements for frontier models, making them among the main actors responsible for mitigating misalignment. They are primarily responsible because they have high capability to address the issue (e.g., regulations using audits or licenses), substantial causal influence (e.g., through economic and intellectual investment as well as infrastructure provision), and widely-agreed obligation to address issues and risks related to national security and public health.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"AI governance actors are the only actors which can efficiently defuse race dynamics (particularly internationally) and provide strong safety requirements for frontier models, and therefore are some of the main actors responsible for mitigating misalignment."</li>                    <li>"AI Governance Actors are primarily responsible because they have:
1. a high degree of capability to address the issue (e.g., regulations using audits or licenses)
2. a substantial causal influence (e.g., through economic and intellectual investment as well as infrastructure provision)
3. a widely-agreed obligation to address issues and risks related to national security and public health"</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Responsibility</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary:</strong> Only one expert commented: "I continue to assess that AI governance actors are 'minimally responsible'. I think a category error is at work here. Saying AI governance actors are responsible for AI risks is like saying that judges are responsible for crimes being committed. The kind of responsibility a judge has is very different from the kind of responsibility that a criminal or a lock-pick-maker has. The specific question asks how responsible should the AI governance actor be for addressing the risk. In almost all cases, the AI governance actor is responsible for requiring some other actor to address the risk, not addressing the risk themselves. The better way to think of this is that the AI governance actor is responsible for holding responsible the actor who is properly responsible. It would be recursive that AI governance actor is themselves responsible. Would we propose some meta-AI-goverance governor who holds responsible the AI governance actors that fail to hold responsible the actors that should be responsible? This is not the right way of thinking. We can rightly say that AI governance actors are responsible for some meta issues, like ensuring that Governments are properly informed etc."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"I continue to assess that AI governance actors are "minimally responsible". I think a category error is at work here. Saying AI governance actors are responsible for AI risks is like saying that judges are responsible for crimes being committed. The kind of responsibility a judge has is very different from the kind of responsibility that a criminal or a lock-pick-maker has. The specific question asks how responsible should the AI governance actor be for addressing the risk. In almost all cases, the AI governance actor is responsible for requiring some other actor to address the risk, not addressing the risk themselves. The better way to think of this is that the AI governance actor is responsible for holding responsible the actor who is properly responsible. It would be recursive that AI governance actor is themselves responsible. Would we propose some meta-AI-goverance governor who holds responsible the AI governance actors that fail to hold responsible the actors that should be responsible? This is not the right way of thinking. We can rightly say that AI governance actors are responsible for some meta issues, like ensuring that Governments are properly informed etc."</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const pills = document.querySelectorAll('.nav-pill');
            const sections = document.querySelectorAll('.actor-section');

            pills.forEach(pill => {
                pill.addEventListener('click', function() {
                    // Remove active class from all pills and sections
                    pills.forEach(p => p.classList.remove('active'));
                    sections.forEach(s => s.classList.remove('active'));

                    // Add active class to clicked pill
                    this.classList.add('active');

                    // Show corresponding section
                    const targetId = this.getAttribute('data-target');
                    const targetSection = document.getElementById(targetId);
                    if (targetSection) {
                        targetSection.classList.add('active');
                    }
                });
            });
        });
    </script>
</body>
</html>
