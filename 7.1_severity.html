<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>7.1 AI pursuing its own goals in conflict with human goals or values - Business as usual Scenario</title>
    <link href="https://fonts.googleapis.com/css2?family=Figtree:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Figtree', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background-color: #ffffff;
            color: #000000;
            line-height: 1.3;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;
        }

        h1 {
            text-align: center;
            margin-bottom: 8px;
            color: #000000;
            font-weight: 600;
            font-size: 18px;
        }

        .selection-title {
            text-align: center;
            font-size: 14px;
            font-weight: 600;
            color: #666666;
            margin-bottom: 10px;
        }

        .nav-pills {
            display: flex;
            flex-wrap: wrap;
            gap: 4px;
            margin-bottom: 15px;
            justify-content: center;
        }

        .nav-pill {
            background: #f8f9fa;
            border: 1px solid #e0e0e0;
            border-radius: 25px;
            padding: 12px 20px;
            cursor: pointer;
            font-family: 'Figtree', sans-serif;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            color: #000000;
        }

        .nav-pill:hover {
            background: #e9ecef;
            border-color: #000000;
        }

        .nav-pill.active {
            background: #a32035;
            color: white;
            border-color: #a32035;
        }

        .tab-section {
            display: none;
        }

        .tab-section.active {
            display: block;
        }

        .content-box {
            background: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 15px;
        }

        .criteria-header {
            font-size: 15px;
            font-weight: 600;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #a32035;
            color: #a32035;
        }

        .summary-section {
            margin-bottom: 20px;
        }

        .summary-text {
            margin-bottom: 15px;
            font-weight: 500;
            color: #000000;
            font-size: 15px;
        }

        .quote-details {
            margin-top: 15px;
        }

        .quote-toggle {
            cursor: pointer;
            color: #000000;
            font-weight: 500;
            font-size: 14px;
            padding: 8px 0;
            border-bottom: 1px dotted #a32035;
            display: inline-block;
        }

        .quote-toggle:hover {
            color: #333333;
        }

        .quote-list {
            margin-top: 15px;
            padding-left: 20px;
        }

        .quote-list li {
            margin-bottom: 12px;
            font-size: 12px;
            line-height: 1.3;
            color: #000000;
        }

        @media (max-width: 768px) {
            .nav-pill {
                font-size: 10px;
                padding: 4px 8px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>7.1 AI pursuing its own goals in conflict with human goals or values - Business as usual Scenario</h1>

        <div class="selection-title">Select a category:</div>
        <div class="nav-pills">
            <button class="nav-pill active" data-target="reasoning">
                Reasoning
            </button>
            <button class="nav-pill" data-target="other">
                Other
            </button>
        </div>

        <div class="content-sections">
            <div class="tab-section active" id="reasoning">
                <div class="content-box">
                    <h3 class="criteria-header">Reasoning</h3>
                    <div class="summary-section">
                        <p class="summary-text"><strong>AI-Generated Summary of Expert Comments:</strong> Concerned experts predict substantial to severe harm, with one noting frontier models by 2030 could autonomously complete 40-80 hour engineering tasks, enabling "sophisticated, sustained cyberattacks on critical infrastructure." Deceptive capabilities are "well demonstrated" while appropriate controls aren't widely adopted. Under Pragmatic Mitigations, skepticism dominates. Multiple experts doubt current pragmatic approaches are sufficient - one states they'll reduce risk "by a relatively small amount," another notes "we have no unified theory" for understanding or mitigating goal emergence. Value alignment research and human-in-command controls are proposed but seen as insufficient. One expert warns that even non-catastrophic misalignment could cause billions in losses across many agents. However, some argue AI lacks true goals or sentience - one states harm comes from "human values imprinted into the model, not because AI has its own goals." There is therefore disagreement between those who see this as unlikely but potentially "rapidly catastrophic" if it occurs, versus those who view the risk as implausible with current architectures.</p>

                        <details class="quote-details">
                            <summary class="quote-toggle">See all expert comments (12)</summary>
                            <ul class="quote-list">
                                <li>"I dont think we have AI tools that are so advanced that they can create their own goals yet! We may have some existing in private servers or private testing in big gen-purp AI labs but i dont think they are going to be released to the public."</li>
                                <li>"Justification:

Mitigations reduce catastrophic probability, but severe value-misalignment remains probable due to complexity and speed of model development

Substantial harm still likely from side effects, goal drift, and misinterpretation of human intent

Catastrophic risk still non-trivial, particularly in systems with latent autonomy and open-ended optimization"</li>
                                <li>"Increasingly autonomous and agentic AI systems may act in ways misaligned with human intent due to inadequate oversight, poor goal specification, or emergent behavior. Substantial to severe harms are most probable, including systems optimizing for proxy objectives that conflict with ethical or societal values. Catastrophic harm, though unlikely in the near term, remains possible if powerful AI models operate without human intervention in critical infrastructure or defense contexts. With pragmatic mitigations such as value alignment research, rigorous model interpretability, human-in-command controls, and continuous red-teaming, these risks reduce significantly. Most harms shift to minor or substantial, with safety frameworks and monitoring ensuring AI actions remain traceable, reversible, and consistent with human goals."</li>
                                <li>"Benchmark extrapolations and time-horizon studies show that by 2030 we can conservatively expect frontier models to be able to reliably autonomously complete tasks that would take a human software engineer ~40-80 hours, representing unrepresented capability for misaligned agents to execute catastrophic actions such as sophisticated, sustained cyberattacks on critical infrastructure. There are compelling empirical and theoretical reasons to expect with BAU for such a misalignment to occur and remain undetected during the first critical period, making catastrophic risk quite likely. 

With Pragmatic Mitigations, and especially AI control and other scheming detection methods, it is possible this catastrophic risk could be mitigated before recursive self-improvement. Much of the reduction of risk in this world comes from slowing down AI progress and governance measures to force implementation of things like basic internal security at labs."</li>
                                <li>"With the autonomy of generative AI systems increasing, without sufficient safeguards, especially with advent of physical AI systems, we could be looking at significant and irreparable harm due to misalignment with human values."</li>
                                <li>"So far, we dont have a unified theory to study whether AI will generate self-models and derive independent interests or goals from them. Therefore, we have no evidence that we have a way to mitigate it, even though I believe that mitigation still has some effect."</li>
                                <li>"Industry is in a rush to create more capable systems before making them safe.  Alignment is tricky, whereas scaling capability seems possible simply with more data and compute.  Deceptive capabilities of models are well demonstrated, and appropriate system controls are not widely adopted."</li>
                                <li>"I've rated the substantial harm caused by AI pursuing its own goals in conflict with human goals or values lower because despite having seen the recent Grok nazi comments, I define "its own goals" as a form of sentience, which I do not ascribe to generative AI. Harm is likely to occur, but that will be because of certain human values imprinted into the model from the training data or prompts, not because AI was pursuing its own goals (it does not have any)."</li>
                                <li>"I think that applying the types of pragmatic mitigations currently in the overton window will only reduce the likelihood of substantial (and higher) levels of harm by a relatively small amount and mitigations that would not currently be considered pragmatic are required for more significant risk reductions"</li>
                                <li>"Unsure how much of this self-corrects in the market, i.e. people don't want to use agents that have risk of doing the thing you don't want. Even minor non-catastrophic misalignment can have impact of $Ms likely, and across many agents in an economy maybe you get to $bns.

But things aren't looking great on alignment either, so you might get really catastrophic outcomes that are way beyond the right hand side of the current scale."</li>
                                <li>"This is unlikely, but to the extent that it has any impact beyond quickly-halted failures, it is likely to be rapidly or immediately catastrophic."</li>
                                <li>"If the current AI architecture, structure, and scale-up principle are followed without any substantial change in designing and developing the AI models, it is not possible to reduce the risk of AI pursuing its own goals."</li>
                            </ul>
                        </details>
                    </div>
                </div>
            </div>

            <div class="tab-section" id="other">
                <div class="content-box">
                    <h3 class="criteria-header">Other</h3>
                    <div class="summary-section">
                        <p class="summary-text"><strong>AI-Generated Summary of Expert Comments:</strong> Experts emphasize timing and probability considerations. Several note this risk is binary - "either it happens and it's very bad, or it doesn't." One expert contrasts two scenarios: rather than a single superintelligent AI, they expect "many narrow, proto-agentic AIs" creating numerous smaller misalignments rather than one catastrophic event. Most expect risks to materialize in later years when capabilities are more advanced, with AGI-level technology seen as "relatively unlikely (but not impossible)" in the 5-year timeframe.</p>

                        <details class="quote-details">
                            <summary class="quote-toggle">See all expert comments (6)</summary>
                            <ul class="quote-list">
                                <li>"I think this turns mostly on likelihood not severity. Either it happens and its very bad, or it doesnt happen."</li>
                                <li>"I think that "Business as Usual" would be shamefully reckless, but I also don't think it's very likely. For both questions, I expect most of the risk/harms to occur 4-5 years from now, when AI capabilities are significantly more advanced than they currently are."</li>
                                <li>"My answers are under the assumption that we won't solve for alignment in the next 5 years"</li>
                                <li>"I think the really catastrophic scenarios here are predicated on AGI or beyond-level technology being achieved, which I think is relatively unlikely (but not impossible) in the 5 year timeframe."</li>
                                <li>"I think contrary to the risk model of a single, superintelligent AI, within the next five years, we're more likely to see a growing ecosystem of many narrow, "proto-agentic" AIs designed to autonomously optimize specific, narrow goals (e.g., maximize profit, increase user engagement, manage a power grid). This can create many smaller-scale misalignments that cause tangible economic and social damage. Hence, my risk risk distribution is less polarasing than it would be if expecting a single superintelligence (where there can be a more binary outcome of either the system is aligned, or it is catastrophically not)."</li>
                                <li>"My risk of catastrophic harm in both Business As Usual and Pragmatic Mitigations would be higher, but I also have longer decades+ timelines."</li>
                            </ul>
                        </details>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const pills = document.querySelectorAll('.nav-pill');
            const sections = document.querySelectorAll('.tab-section');

            pills.forEach(pill => {
                pill.addEventListener('click', function() {
                    pills.forEach(p => p.classList.remove('active'));
                    sections.forEach(s => s.classList.remove('active'));

                    this.classList.add('active');

                    const targetId = this.getAttribute('data-target');
                    const targetSection = document.getElementById(targetId);
                    if (targetSection) {
                        targetSection.classList.add('active');
                    }
                });
            });
        });
    </script>
</body>
</html>
