<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>7.4 Lack of transparency or interpretability - Vulnerability (Actors)</title>
    <link href="https://fonts.googleapis.com/css2?family=Figtree:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Figtree', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background-color: #ffffff;
            color: #000000;
            line-height: 1.3;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;
        }

        h1 {
            text-align: center;
            margin-bottom: 8px;
            color: #000000;
            font-weight: 600;
            font-size: 18px;
        }

        .selection-title {
            text-align: center;
            font-size: 14px;
            font-weight: 600;
            color: #666666;
            margin-bottom: 10px;
        }

        .nav-pills {
            display: flex;
            flex-wrap: wrap;
            gap: 4px;
            margin-bottom: 15px;
            justify-content: center;
        }

        .nav-pill {
            background: #f8f9fa;
            border: 1px solid #e0e0e0;
            border-radius: 25px;
            padding: 12px 20px;
            cursor: pointer;
            font-family: 'Figtree', sans-serif;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            color: #000000;
        }

        .nav-pill:hover {
            background: #e9ecef;
            border-color: #000000;
        }

        .nav-pill.active {
            background: #000000;
            color: white;
            border-color: #000000;
        }

        .entity-section {
            display: none;
        }

        .entity-section.active {
            display: block;
        }

        .content-grid {
            display: flex;
            width: 100%;
            gap: 4px;
        }

        .content-column {
            background: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;
        }

        .criteria-header {
            font-size: 12px;
            font-weight: 600;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid;
        }

        .criteria-header.higher {
            color: #FF0000;
            border-bottom-color: #FF0000;
        }

        .criteria-header.lower {
            color: #2E5C8A;
            border-bottom-color: #2E5C8A;
        }

        .summary-section {
            margin-bottom: 20px;
        }

        .summary-text {
            margin-bottom: 15px;
            font-weight: 500;
            color: #000000;
            font-size: 15px;
        }

        .quote-details {
            margin-top: 15px;
        }

        .quote-toggle {
            cursor: pointer;
            color: #000000;
            font-weight: 500;
            font-size: 10px;
            padding: 8px 0;
            border-bottom: 1px dotted #000000;
            display: inline-block;
        }

        .quote-toggle:hover {
            color: #333333;
        }

        .quote-list {
            margin-top: 15px;
            padding-left: 20px;
        }

        .quote-list li {
            margin-bottom: 12px;
            font-size: 10px;
            line-height: 1.3;
            color: #000000;
        }

        @media (max-width: 768px) {
            .content-grid {
                gap: 4px;
            }

            .selection-title {
                text-align: center;
                font-size: 14px;
                font-weight: 600;
                color: #666666;
                margin-bottom: 10px;
            }

            .nav-pills {
                justify-content: flex-start;
            }

            .nav-pill {
                font-size: 10px;
                padding: 4px 8px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>7.4 Lack of transparency or interpretability - Vulnerability (Actors)</h1>

        <div class="selection-title">Select a actor:</div>
        <div class="nav-pills">
            <button class="nav-pill active" data-target="AIDeveloperGeneralpurposeAI">
                AI Developer (General-purpose AI)
            </button>
            <button class="nav-pill" data-target="AIDeveloperSpecializedAI">
                AI Developer (Specialized AI)
            </button>
            <button class="nav-pill" data-target="AIDeployer">
                AI Deployer
            </button>
            <button class="nav-pill" data-target="AIInfrastructureProvider">
                AI Infrastructure Provider
            </button>
            <button class="nav-pill" data-target="AIUser">
                AI User
            </button>
            <button class="nav-pill" data-target="AffectedStakeholder">
                Affected Stakeholder
            </button>
            <button class="nav-pill" data-target="AIGovernanceActor">
                AI Governance Actor
            </button>
        </div>

        <div class="content-sections">
            <div class="entity-section active" id="AIDeveloperGeneralpurposeAI">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "These actors face maximum vulnerability as lack of transparency directly undermines their ability to explain model behavior, debug systems, and build user trust. The opacity of general-purpose AI models creates significant reputational and liability risks."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Developer (General-purpose AI) - Extremely vulnerable: These actors face maximum vulnerability as lack of transparency directly undermines their ability to explain model behavior, debug systems, and build user trust. The opacity of general-purpose AI models creates significant reputational and liability risks.
edural fairness and contestability rights."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> AI developers (both general-purpose and specialized) are minimally vulnerable as they are not the ones impacted by lack of transparency. Developers are at risk only to the extent they don't know how their models work, in which case their risk is in being able to improve them (assuming developers test accuracy even if they don't have transparency on how models work and don't fraudulently overstate capabilities). One expert commented: "Ai developers are the ones deciding if developing a black box or a transparent model and they have all the information regarding the model, the data, the users, etc. There's a clear information asymmetry"</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (3)</summary>
                <ul class="quote-list">
                    <li>"AI developers (both general purpose, and specialized) are minimally vulnerable as they are not the ones impacted by lack of transparency. Lack of sufficient explainability could impact them due to potential emergent misalignment issues but those risks are covered independently."</li>                    <li>"Developers are at risk only to the extent they don't know how their models work, in which case their risk is in being able to improve them. This assumes the developers test the accuracy of their models,  even if they don't have transparency on how the models work, and don't fraudulently overstate their capabilities."</li>                    <li>"Ai developers are the ones deciding if developing a black box or a transparent model and they have all the information regarding the model, the data, the users, etc. There's a clear information asymmetry"</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIDeveloperSpecializedAI">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "While somewhat more interpretable than general-purpose systems, specialized AI developers still face substantial challenges in explaining domain-specific models to stakeholders and ensuring auditability."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Developer (Specialized AI) - Highly vulnerable: While somewhat more interpretable than general-purpose systems, specialized AI developers still face substantial challenges in explaining domain-specific models to stakeholders and ensuring auditability."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> AI developers (both general-purpose and specialized) are minimally vulnerable as they are not the ones impacted by lack of transparency. Lack of sufficient explainability could impact them due to potential emergent misalignment issues but those risks are covered independently. One expert finds it bizarre that consensus suggests AI developers are vulnerable to this risk, thinking it exceedingly unlikely they will find themselves in regulatory regimes where this intransparency is of any consequence to them.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"AI developers (both general purpose, and specialized) are minimally vulnerable as they are not the ones impacted by lack of transparency. Lack of sufficient explainability could impact them due to potential emergent misalignment issues but those risks are covered independently."</li>                    <li>"In particular I find the expert consensus that AI developers (both general purpose and specialized) are vulnerable to this risk bizarre: I think it is exceedingly unlikely they will find themselves in regulatory regimes where this intransparency is of any consequence to them."</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIDeployer">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Deployers must operationalize AI systems without full understanding of decision-making processes, creating compliance risks and difficulty in meeting explainability requirements for regulated industries. This limits their ability to validate AI outputs or ensure accountability."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Deployer - Highly vulnerable: Deployers must operationalize AI systems without full understanding of decision-making processes, creating compliance risks and difficulty in meeting explainability requirements for regulated industries. This limits their ability to validate AI outputs or ensure accountability."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "I lowered AI Deployers from Extremely Vulnerable to Highly Vulnerable as whilst they might have limited control on AI system inner workings, they do have the ability and obligation to test AI systems before exposing the technology to users"</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"I lowered AI Deployers from Extremely Vulnerable to Highly Vulnerable as whilst they might have limited control on AI system inner workings, they do have the ability and obligation to test AI systems before exposing the technology to users"</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIInfrastructureProvider">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "AI Infrastructure Provider - Minimally vulnerable: Infrastructure providers primarily supply computational resources rather than model logic, thus experiencing limited direct impact from model interpretability issues."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI Infrastructure Provider - Minimally vulnerable: Infrastructure providers primarily supply computational resources rather than model logic, thus experiencing limited direct impact from model interpretability issues."</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIUser">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "AI User - Highly vulnerable: Users face significant vulnerability through inability to understand AI recommendations, verify outputs, or identify errors and biases. This creates risks of misplaced trust, inappropriate reliance, and inability to exercise informed judgment."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"AI User - Highly vulnerable: Users face significant vulnerability through inability to understand AI recommendations, verify outputs, or identify errors and biases. This creates risks of misplaced trust, inappropriate reliance, and inability to exercise informed judgment."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AffectedStakeholder">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Stakeholders impacted by AI decisions have limited recourse when they cannot understand how decisions affecting them were made, undermining procedural fairness and contestability rights."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Affected Stakeholder - Highly vulnerable: Stakeholders impacted by AI decisions have limited recourse when they cannot understand how decisions affecting them were made, undermining procedural fairness and contestability rights."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIGovernanceActor">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> AI Governance Actors are vulnerable due to their oversight responsibilities for technologies that may exceed their technical expertise, making them susceptible to manipulation and influence from special interest groups. Their vulnerability stems from indirect exposure - while they don't operate AI systems directly, unpredictable system behavior can lead to reputational harm, policy failures, and regulations that inadequately address robustness concerns.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (3)</summary>
                <ul class="quote-list">
                    <li>"AI Governance Actors - Extremely Vulnerable: Governance is tasked to make decisions. If those decisions are based on extremely incorrect assumptions owing to the lack of information about the AI systems, or the inability to compare and analyze them, then those decisions are worthless."</li>                    <li>"AI Governance Actor - Extremely vulnerable: Governance actors cannot effectively regulate, audit, or establish standards for systems they cannot interpret. Lack of transparency fundamentally impairs their core function of oversight, risk assessment, and policy development."</li>                    <li>"I upgraded my rating for vulnerability for AI governance actors, based on lack of transparency from AI developers affecting these governance actors ability to achieve their goals."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const pills = document.querySelectorAll('.nav-pill');
            const sections = document.querySelectorAll('.entity-section');

            pills.forEach(pill => {
                pill.addEventListener('click', function() {
                    pills.forEach(p => p.classList.remove('active'));
                    sections.forEach(s => s.classList.remove('active'));

                    this.classList.add('active');

                    const targetId = this.getAttribute('data-target');
                    const targetSection = document.getElementById(targetId);
                    if (targetSection) {
                        targetSection.classList.add('active');
                    }
                });
            });
        });
    </script>
</body>
</html>
