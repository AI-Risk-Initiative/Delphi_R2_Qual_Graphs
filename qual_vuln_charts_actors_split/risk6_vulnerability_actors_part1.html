<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3.2 False or misleading information - Vulnerability (Actors) - Part 1</title>
    <link href="https://fonts.googleapis.com/css2?family=Figtree:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Figtree', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background-color: #ffffff;
            color: #000000;
            line-height: 1.3;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;
        }

        h1 {
            text-align: center;
            margin-bottom: 8px;
            color: #000000;
            font-weight: 600;
            font-size: 18px;
        }

        .selection-title {
            text-align: center;
            font-size: 14px;
            font-weight: 600;
            color: #666666;
            margin-bottom: 10px;
        }

        .nav-pills {
            display: flex;
            flex-wrap: wrap;
            gap: 4px;
            margin-bottom: 15px;
            justify-content: center;
        }

        .nav-pill {
            background: #f8f9fa;
            border: 1px solid #e0e0e0;
            border-radius: 25px;
            padding: 12px 20px;
            cursor: pointer;
            font-family: 'Figtree', sans-serif;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            color: #000000;
        }

        .nav-pill:hover {
            background: #e9ecef;
            border-color: #000000;
        }

        .nav-pill.active {
            background: #000000;
            color: white;
            border-color: #000000;
        }

        .entity-section {
            display: none;
        }

        .entity-section.active {
            display: block;
        }

        .content-grid {
            display: flex;
            width: 100%;
            gap: 4px;
        }

        .content-column {
            background: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;
        }

        .criteria-header {
            font-size: 12px;
            font-weight: 600;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid;
        }

        .criteria-header.higher {
            color: #FF0000;
            border-bottom-color: #FF0000;
        }

        .criteria-header.lower {
            color: #2E5C8A;
            border-bottom-color: #2E5C8A;
        }

        .summary-section {
            margin-bottom: 20px;
        }

        .summary-text {
            margin-bottom: 15px;
            font-weight: 500;
            color: #000000;
            font-size: 15px;
        }

        .quote-details {
            margin-top: 15px;
        }

        .quote-toggle {
            cursor: pointer;
            color: #000000;
            font-weight: 500;
            font-size: 10px;
            padding: 8px 0;
            border-bottom: 1px dotted #000000;
            display: inline-block;
        }

        .quote-toggle:hover {
            color: #333333;
        }

        .quote-list {
            margin-top: 15px;
            padding-left: 20px;
        }

        .quote-list li {
            margin-bottom: 12px;
            font-size: 10px;
            line-height: 1.3;
            color: #000000;
        }

        @media (max-width: 768px) {
            .content-grid {
                gap: 4px;
            }

            .selection-title {
                text-align: center;
                font-size: 14px;
                font-weight: 600;
                color: #666666;
                margin-bottom: 10px;
            }

            .nav-pills {
                justify-content: flex-start;
            }

            .nav-pill {
                font-size: 10px;
                padding: 4px 8px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>3.2 False or misleading information - Vulnerability (Actors) - Part 1</h1>

        <div class="selection-title">Select a actor:</div>
                <div class="nav-pills">
            <button class="nav-pill active" data-target="AIDeveloperGeneralpurposeAI">
                AI Developer (General-purpose AI)
            </button>
            <button class="nav-pill" data-target="AIDeployer">
                AI Deployer
            </button>
            <button class="nav-pill" data-target="AIGovernanceActor">
                AI Governance Actor
            </button>
            <button class="nav-pill" data-target="AIUser">
                AI User
            </button>
        </div>

                <div class="content-sections">
<div class="entity-section active" id="AIDeveloperGeneralpurposeAI">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> All actors in the ecosystem are vulnerable to the information supply chain and disinformation kill chains. Developers have software dependencies already being disrupted by information breakdown—open source projects struggle under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, and fake news. Even actors who fill every role are vulnerable to the very issue they're creating. Some experts updated vulnerability higher due to risks like lawsuits, with certain countries or regions like the EU potentially enforcing bans, requiring product changes, or imposing major fines, representing meaningful moderate risks.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"Those who marked any actor as invulnerable to this issue have not thought through the information supply chain and disinformation kill chains.   Some listed infrastructure and providers as not vulnerable, but all of the developers have have software dependencies, and those are already being disrupted by information breakdown.  How many open source projects are depended on by major corporations and governments, and are now struggling under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, fake news.   How many AI policies are being influenced by false information?  All of them? 

If you search on Google for the latest models like Veo3 from Google, the search results are filled with scam sites pretending to be that model.  So even an actor who fills every role in the ecosystem is vulnerable to the very issue its creating. 

It may not be consensus, but I stand by my rating that every actor is at least highly vulnerable,  none of them have a universal verifier or large scale fact checking service, which means their own devs, researchers, executives and AIs are nearly as vulnerable as the users to this risk"</li>                    <li>"I updated vulnerability for some developers and deployers, making it higher. I think the biggest risk here is things like lawsuits. We already have widespread misinformation and acceptance of misinformation, so in many cases companies are not substantially penalized for being part of this problem. But there are certain countries or regions like the EU that might enforce bans, require product changes, impose major fines. These punishments aren't game breaking, but could be meaningful/moderate risks.

On the other hand, I'm still skeptical that affected stakeholders are extremely vulnerable. People have limited information diets, low levels of knowledge, and are already susceptible to misinformation from mainstream sources (e.g., TV news). So the extra misinformation harm actually has a ceiling (it doesn't change as much as you might think)."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
<div class="entity-section" id="AIDeployer">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> All actors in the ecosystem are vulnerable to the information supply chain and disinformation kill chains, with software dependencies already being disrupted by information breakdown. Even actors who fill every role are vulnerable to the very issue they're creating. Some experts updated vulnerability for deployers higher due to risks like lawsuits, with certain countries or regions like the EU potentially enforcing bans, requiring product changes, or imposing major fines.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"Those who marked any actor as invulnerable to this issue have not thought through the information supply chain and disinformation kill chains.   Some listed infrastructure and providers as not vulnerable, but all of the developers have have software dependencies, and those are already being disrupted by information breakdown.  How many open source projects are depended on by major corporations and governments, and are now struggling under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, fake news.   How many AI policies are being influenced by false information?  All of them? 

If you search on Google for the latest models like Veo3 from Google, the search results are filled with scam sites pretending to be that model.  So even an actor who fills every role in the ecosystem is vulnerable to the very issue its creating. 

It may not be consensus, but I stand by my rating that every actor is at least highly vulnerable,  none of them have a universal verifier or large scale fact checking service, which means their own devs, researchers, executives and AIs are nearly as vulnerable as the users to this risk"</li>                    <li>"I updated vulnerability for some developers and deployers, making it higher. I think the biggest risk here is things like lawsuits. We already have widespread misinformation and acceptance of misinformation, so in many cases companies are not substantially penalized for being part of this problem. But there are certain countries or regions like the EU that might enforce bans, require product changes, impose major fines. These punishments aren't game breaking, but could be meaningful/moderate risks.

On the other hand, I'm still skeptical that affected stakeholders are extremely vulnerable. People have limited information diets, low levels of knowledge, and are already susceptible to misinformation from mainstream sources (e.g., TV news). So the extra misinformation harm actually has a ceiling (it doesn't change as much as you might think)."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
<div class="entity-section" id="AIGovernanceActor">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> All actors in the ecosystem are vulnerable to the information supply chain and disinformation kill chains. Governance actors face disruption from false information—all AI policies are being influenced by false information. Every actor is at least highly vulnerable; none have universal verifiers or large-scale fact checking services, meaning governance actors' own devs, researchers, executives and AIs are nearly as vulnerable as users to this risk.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Those who marked any actor as invulnerable to this issue have not thought through the information supply chain and disinformation kill chains.   Some listed infrastructure and providers as not vulnerable, but all of the developers have have software dependencies, and those are already being disrupted by information breakdown.  How many open source projects are depended on by major corporations and governments, and are now struggling under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, fake news.   How many AI policies are being influenced by false information?  All of them? 

If you search on Google for the latest models like Veo3 from Google, the search results are filled with scam sites pretending to be that model.  So even an actor who fills every role in the ecosystem is vulnerable to the very issue its creating. 

It may not be consensus, but I stand by my rating that every actor is at least highly vulnerable,  none of them have a universal verifier or large scale fact checking service, which means their own devs, researchers, executives and AIs are nearly as vulnerable as the users to this risk"</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
<div class="entity-section" id="AIUser">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Affected Stakeholders / AI Users: Extremely vulnerable because they are suffered from misleading information."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Affected Stakeholders / AI Users: Extremely vulnerable because they are suffered from misleading information."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIDeveloperSpecializedAI">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Specialized AI Developers: Highly vulnerable because they forcus on specialized domains and can cause acute harm. They are also less resourced for large-scale context oversight."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Specialized AI Developers: Highly vulnerable because they forcus on specialized domains and can cause acute harm. They are also less resourced for large-scale context oversight."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIDeployer">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> All actors in the ecosystem are vulnerable to the information supply chain and disinformation kill chains, with software dependencies already being disrupted by information breakdown. Even actors who fill every role are vulnerable to the very issue they're creating. Some experts updated vulnerability for deployers higher due to risks like lawsuits, with certain countries or regions like the EU potentially enforcing bans, requiring product changes, or imposing major fines.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"Those who marked any actor as invulnerable to this issue have not thought through the information supply chain and disinformation kill chains.   Some listed infrastructure and providers as not vulnerable, but all of the developers have have software dependencies, and those are already being disrupted by information breakdown.  How many open source projects are depended on by major corporations and governments, and are now struggling under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, fake news.   How many AI policies are being influenced by false information?  All of them? 

If you search on Google for the latest models like Veo3 from Google, the search results are filled with scam sites pretending to be that model.  So even an actor who fills every role in the ecosystem is vulnerable to the very issue its creating. 

It may not be consensus, but I stand by my rating that every actor is at least highly vulnerable,  none of them have a universal verifier or large scale fact checking service, which means their own devs, researchers, executives and AIs are nearly as vulnerable as the users to this risk"</li>                    <li>"I updated vulnerability for some developers and deployers, making it higher. I think the biggest risk here is things like lawsuits. We already have widespread misinformation and acceptance of misinformation, so in many cases companies are not substantially penalized for being part of this problem. But there are certain countries or regions like the EU that might enforce bans, require product changes, impose major fines. These punishments aren't game breaking, but could be meaningful/moderate risks.

On the other hand, I'm still skeptical that affected stakeholders are extremely vulnerable. People have limited information diets, low levels of knowledge, and are already susceptible to misinformation from mainstream sources (e.g., TV news). So the extra misinformation harm actually has a ceiling (it doesn't change as much as you might think)."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIInfrastructureProvider">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Infrastructure Providers: Moderately vulnerable because they indirectly decide information quality." Another noted all actors are vulnerable to information supply chain and disinformation kill chains—infrastructure providers have software dependencies already being disrupted by information breakdown, with open source projects struggling under false AI bug reports, incorrect guidance, and fake news. Every actor is at least highly vulnerable; none have universal verifiers or large-scale fact checking services.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (2)</summary>
                <ul class="quote-list">
                    <li>"Infrastructure Providers: Moderately vulnerable because they indirectly decide information quality."</li>                    <li>"Those who marked any actor as invulnerable to this issue have not thought through the information supply chain and disinformation kill chains.   Some listed infrastructure and providers as not vulnerable, but all of the developers have have software dependencies, and those are already being disrupted by information breakdown.  How many open source projects are depended on by major corporations and governments, and are now struggling under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, fake news.   How many AI policies are being influenced by false information?  All of them? 

If you search on Google for the latest models like Veo3 from Google, the search results are filled with scam sites pretending to be that model.  So even an actor who fills every role in the ecosystem is vulnerable to the very issue its creating. 

It may not be consensus, but I stand by my rating that every actor is at least highly vulnerable,  none of them have a universal verifier or large scale fact checking service, which means their own devs, researchers, executives and AIs are nearly as vulnerable as the users to this risk"</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIUser">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Affected Stakeholders / AI Users: Extremely vulnerable because they are suffered from misleading information."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Affected Stakeholders / AI Users: Extremely vulnerable because they are suffered from misleading information."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AffectedStakeholder">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> One expert commented: "Affected Stakeholders / AI Users: Extremely vulnerable because they are suffered from misleading information."</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Affected Stakeholders / AI Users: Extremely vulnerable because they are suffered from misleading information."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> Some experts are skeptical that affected stakeholders are extremely vulnerable. People have limited information diets, low levels of knowledge, and are already susceptible to misinformation from mainstream sources (e.g., TV news). The extra misinformation harm actually has a ceiling—it doesn't change as much as you might think, as people are already dealing with widespread misinformation and acceptance of it.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"I updated vulnerability for some developers and deployers, making it higher. I think the biggest risk here is things like lawsuits. We already have widespread misinformation and acceptance of misinformation, so in many cases companies are not substantially penalized for being part of this problem. But there are certain countries or regions like the EU that might enforce bans, require product changes, impose major fines. These punishments aren't game breaking, but could be meaningful/moderate risks.

On the other hand, I'm still skeptical that affected stakeholders are extremely vulnerable. People have limited information diets, low levels of knowledge, and are already susceptible to misinformation from mainstream sources (e.g., TV news). So the extra misinformation harm actually has a ceiling (it doesn't change as much as you might think)."</li>
                </ul>
            </details>
                        </div>
                    </div>
                </div>
            </div>
            <div class="entity-section" id="AIGovernanceActor">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> All actors in the ecosystem are vulnerable to the information supply chain and disinformation kill chains. Governance actors face disruption from false information—all AI policies are being influenced by false information. Every actor is at least highly vulnerable; none have universal verifiers or large-scale fact checking services, meaning governance actors' own devs, researchers, executives and AIs are nearly as vulnerable as users to this risk.</p>

            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (1)</summary>
                <ul class="quote-list">
                    <li>"Those who marked any actor as invulnerable to this issue have not thought through the information supply chain and disinformation kill chains.   Some listed infrastructure and providers as not vulnerable, but all of the developers have have software dependencies, and those are already being disrupted by information breakdown.  How many open source projects are depended on by major corporations and governments, and are now struggling under false AI bug reports, AI clone projects, incorrect guidance material, incorrect search results, fake news.   How many AI policies are being influenced by false information?  All of them? 

If you search on Google for the latest models like Veo3 from Google, the search results are filled with scam sites pretending to be that model.  So even an actor who fills every role in the ecosystem is vulnerable to the very issue its creating. 

It may not be consensus, but I stand by my rating that every actor is at least highly vulnerable,  none of them have a universal verifier or large scale fact checking service, which means their own devs, researchers, executives and AIs are nearly as vulnerable as the users to this risk"</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>AI-generated summary of expert comments:</strong> [NO EXPERT COMMENTS PROVIDED]</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const pills = document.querySelectorAll('.nav-pill');
            const sections = document.querySelectorAll('.entity-section');

            pills.forEach(pill => {
                pill.addEventListener('click', function() {
                    pills.forEach(p => p.classList.remove('active'));
                    sections.forEach(s => s.classList.remove('active'));

                    this.classList.add('active');

                    const targetId = this.getAttribute('data-target');
                    const targetSection = document.getElementById(targetId);
                    if (targetSection) {
                        targetSection.classList.add('active');
                    }
                });
            });
        });
    </script>
</body>
</html>
